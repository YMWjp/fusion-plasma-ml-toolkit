#!/usr/bin/env python3
"""
ãƒãƒƒãƒåˆ†æãƒ„ãƒ¼ãƒ«

raw_dataãƒ•ã‚©ãƒ«ãƒ€å†…ã®è¤‡æ•°ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è‡ªå‹•å‡¦ç†ã—ã€
ãã‚Œãã‚Œã®çµæœã‚’å€‹åˆ¥ã®ãƒ•ã‚©ãƒ«ãƒ€ã«æ ¼ç´ã™ã‚‹
"""

import os
import glob
import shutil
import sys
import datetime
from pathlib import Path
import re

# æ—¢å­˜ã®ãƒ„ãƒ¼ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from smoothing_test import RawDataSmoothingVisualizer
from all_isat_comparison import AllIsatComparator

class BatchAnalyzer:
    """ãƒãƒƒãƒåˆ†æã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, raw_data_dir="raw_data", output_base_dir="results"):
        self.raw_data_dir = raw_data_dir
        self.output_base_dir = output_base_dir
        self.processed_files = []
        self.failed_files = []
        
    def find_data_files(self):
        """raw_dataãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢"""
        print(f"Searching for data files in {self.raw_data_dir}...")
        
        # ä¸€èˆ¬çš„ãªãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œç´¢
        patterns = [
            "*.txt",
            "*.dat", 
            "*DivIis_tor_sum*.txt",
            "*DivIis_tor_sum*.dat"
        ]
        
        data_files = []
        for pattern in patterns:
            search_path = os.path.join(self.raw_data_dir, pattern)
            found_files = glob.glob(search_path)
            data_files.extend(found_files)
        
        # é‡è¤‡ã‚’é™¤å»
        data_files = list(set(data_files))
        
        print(f"Found {len(data_files)} data files:")
        for file_path in data_files:
            file_size = os.path.getsize(file_path) / 1024  # KB
            print(f"  - {os.path.basename(file_path)} ({file_size:.1f} KB)")
        
        return data_files
    
    def extract_shot_number(self, file_path):
        """ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰shotç•ªå·ã‚’æŠ½å‡º"""
        filename = os.path.basename(file_path)
        
        # ä¸€èˆ¬çš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã§shotç•ªå·ã‚’æŠ½å‡º
        patterns = [
            r'@(\d+)_',  # @123456_
            r'@(\d+)\.',  # @123456.
            r'_(\d+)_',   # _123456_
            r'_(\d+)\.',  # _123456.
            r'(\d{6})',   # 6æ¡ã®æ•°å­—
            r'(\d{5})',   # 5æ¡ã®æ•°å­—
        ]
        
        for pattern in patterns:
            match = re.search(pattern, filename)
            if match:
                return match.group(1)
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰æ‹¡å¼µå­ã‚’é™¤ã„ãŸéƒ¨åˆ†ã‚’ä½¿ç”¨
        return os.path.splitext(filename)[0]
    
    def create_output_directory(self, shot_number):
        """å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ"""
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        output_dir = os.path.join(self.output_base_dir, f"shot_{shot_number}_{timestamp}")
        
        os.makedirs(output_dir, exist_ok=True)
        return output_dir
    
    def analyze_single_file(self, file_path, target_columns=None):
        """å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ã®åˆ†æã‚’å®Ÿè¡Œ"""
        # Shotç•ªå·ã‚’æŠ½å‡º
        shot_number = self.extract_shot_number(file_path)
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
        output_dir = self.create_output_directory(shot_number)
        
        # ç¾åœ¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä¿å­˜
        original_dir = os.getcwd()
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’çµ¶å¯¾ãƒ‘ã‚¹ã«å¤‰æ›
        abs_file_path = os.path.abspath(file_path)
        
        try:
            # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ç§»å‹•
            os.chdir(output_dir)
            
            # 1. å…¨ã‚»ãƒ³ã‚µãƒ¼æ¯”è¼ƒåˆ†æï¼ˆå‡ºåŠ›æŠ‘åˆ¶ï¼‰
            comparator = AllIsatComparator(abs_file_path)
            success = comparator.run_comprehensive_analysis(verbose=False)
            
            # 2. å€‹åˆ¥ã‚»ãƒ³ã‚µãƒ¼è©³ç´°åˆ†æï¼ˆä¸Šä½3ã‚»ãƒ³ã‚µãƒ¼ï¼‰- é«˜é€ŸåŒ–ç‰ˆ
            if success and hasattr(comparator, 'analysis_results'):
                # ä¸Šä½ã‚»ãƒ³ã‚µãƒ¼ã‚’å–å¾—
                sensor_scores = {}
                for column in comparator.isat_columns:
                    if column in comparator.analysis_results:
                        stats = comparator.analysis_results[column]['stats']
                        score = stats['max'] * 0.5 + (stats['mean'] / stats['std'] if stats['std'] > 0 else 0) * 0.3
                        sensor_scores[column] = score
                
                # ä¸Šä½3ã‚»ãƒ³ã‚µãƒ¼ã‚’é¸æŠ
                top_sensors = sorted(sensor_scores.items(), key=lambda x: x[1], reverse=True)[:3]
                
                for i, (sensor, score) in enumerate(top_sensors):
                    # å€‹åˆ¥è©³ç´°åˆ†æï¼ˆå‡ºåŠ›æŠ‘åˆ¶ï¼‰
                    visualizer = RawDataSmoothingVisualizer(abs_file_path)
                    if visualizer.load_data():
                        # å‡ºåŠ›ã‚’æŠ‘åˆ¶ã—ã¦é«˜é€Ÿå®Ÿè¡Œ
                        stats_dict, outlier_indices = visualizer.analyze_data_characteristics(sensor)
                        visualizer.apply_smoothing_methods(sensor)
                        
                        # ã‚»ãƒ³ã‚µãƒ¼å›ºæœ‰ã®ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
                        sensor_dir = f"sensor_{sensor.replace('@', '_at_').replace('/', '_')}"
                        os.makedirs(sensor_dir, exist_ok=True)
                        
                        # å¯è¦–åŒ–ã‚’å®Ÿè¡Œï¼ˆå‡ºåŠ›æŠ‘åˆ¶ï¼‰
                        visualizer.visualize_all_comparisons(save_plots=True)
                        
                        # ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚»ãƒ³ã‚µãƒ¼ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ç§»å‹•
                        for plot_file in glob.glob("smoothing_*.png"):
                            new_name = f"{sensor}_{plot_file}"
                            shutil.move(plot_file, os.path.join(sensor_dir, new_name))
            
            # 3. ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆ
            try:
                self.create_summary_report(output_dir, shot_number, abs_file_path, comparator if success else None)
            except Exception as e:
                print(f"Warning: Failed to create summary report: {e}")
            
            self.processed_files.append({
                'file_path': file_path,
                'shot_number': shot_number,
                'output_dir': output_dir,
                'success': success
            })
            
            # ç°¡æ½”ãªå®Œäº†é€šçŸ¥
            print(f"âœ… Shot {shot_number} completed -> {os.path.basename(output_dir)}")
            
        except Exception as e:
            print(f"âŒ Error analyzing {file_path}: {e}")
            self.failed_files.append({
                'file_path': file_path,
                'shot_number': shot_number,
                'error': str(e)
            })
            
        finally:
            # å…ƒã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«æˆ»ã‚‹
            os.chdir(original_dir)
    
    def create_summary_report(self, output_dir, shot_number, file_path, comparator=None):
        """ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆ"""
        report_path = os.path.join(output_dir, "analysis_summary.md")
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(f"# Shot {shot_number} Analysis Summary\n\n")
            f.write(f"**Generated:** {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            f.write(f"**Source File:** `{os.path.basename(file_path)}`\n\n")
            
            if comparator and hasattr(comparator, 'operation_start_idx'):
                # ãƒ—ãƒ©ã‚ºãƒé‹è»¢æœŸé–“æƒ…å ±
                start_time = comparator.data['Time'].iloc[comparator.operation_start_idx]
                end_time = comparator.data['Time'].iloc[comparator.operation_end_idx]
                duration = end_time - start_time
                
                f.write(f"## ğŸš€ Plasma Operation Period\n\n")
                f.write(f"- **Start Time:** {start_time:.3f} s\n")
                f.write(f"- **End Time:** {end_time:.3f} s\n")
                f.write(f"- **Duration:** {duration:.3f} s\n")
                f.write(f"- **Data Points:** {comparator.operation_end_idx - comparator.operation_start_idx + 1}\n\n")
            
            if comparator and hasattr(comparator, 'analysis_results'):
                # ã‚»ãƒ³ã‚µãƒ¼ãƒ©ãƒ³ã‚­ãƒ³ã‚°
                f.write(f"## ğŸ“Š Sensor Ranking\n\n")
                f.write("| Rank | Sensor | Mean | Max | Std Dev | Outliers |\n")
                f.write("|------|--------|------|-----|---------|----------|\n")
                
                # ã‚»ãƒ³ã‚µãƒ¼ã‚’ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆ
                sensor_scores = {}
                for column in comparator.isat_columns:
                    if column in comparator.analysis_results:
                        stats = comparator.analysis_results[column]['stats']
                        score = stats['max'] * 0.5 + (stats['mean'] / stats['std'] if stats['std'] > 0 else 0) * 0.3
                        sensor_scores[column] = (score, stats)
                
                sorted_sensors = sorted(sensor_scores.items(), key=lambda x: x[1][0], reverse=True)
                
                for i, (sensor, (score, stats)) in enumerate(sorted_sensors[:10]):
                    rank_emoji = ["ğŸ¥‡", "ğŸ¥ˆ", "ğŸ¥‰"] if i < 3 else ["ğŸ“"]
                    emoji = rank_emoji[0] if i < 3 else rank_emoji[0]
                    
                    f.write(f"| {emoji} {i+1} | **{sensor}** | {stats['mean']:.6f} | "
                           f"{stats['max']:.6f} | {stats['std']:.6f} | {stats['outlier_count']} |\n")
                
                # æ¨å¥¨äº‹é …
                f.write(f"\n## ğŸ¯ Recommendations\n\n")
                if sorted_sensors:
                    best_sensor = sorted_sensors[0][0]
                    f.write(f"- **Primary sensor:** `{best_sensor}` (highest overall score)\n")
                    if len(sorted_sensors) >= 3:
                        top_3 = [sensor for sensor, _ in sorted_sensors[:3]]
                        f.write(f"- **Top 3 sensors:** {', '.join([f'`{s}`' for s in top_3])}\n")
                
                f.write(f"- **Recommended smoothing:** Adaptive Gaussian (Ïƒ=1.5 base, Ïƒ=3.0 outliers)\n")
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆ
            f.write(f"\n## ğŸ“ Generated Files\n\n")
            for root, dirs, files in os.walk(output_dir):
                for file in files:
                    if file.endswith(('.png', '.csv', '.txt')):
                        rel_path = os.path.relpath(os.path.join(root, file), output_dir)
                        f.write(f"- `{rel_path}`\n")
    
    def run_batch_analysis(self, target_columns=None):
        """ãƒãƒƒãƒåˆ†æã‚’å®Ÿè¡Œ"""
        print("ğŸš€ Starting batch analysis...")
        print(f"ğŸ“ Raw data directory: {self.raw_data_dir}")
        print(f"ğŸ“ Output base directory: {self.output_base_dir}")
        
        # å‡ºåŠ›ãƒ™ãƒ¼ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
        os.makedirs(self.output_base_dir, exist_ok=True)
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
        data_files = self.find_data_files()
        
        if not data_files:
            print("âŒ No data files found!")
            return False
        
        print(f"\nğŸ“Š Processing {len(data_files)} files...")
        
        # å„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ï¼ˆç°¡æ½”ãªé€²æ—è¡¨ç¤ºï¼‰
        for i, file_path in enumerate(data_files, 1):
            print(f"[{i}/{len(data_files)}] {os.path.basename(file_path)}")
            self.analyze_single_file(file_path, target_columns)
        
        # ç·æ‹¬ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆ
        self.create_batch_summary()
        
        print(f"\nğŸ‰ Batch analysis completed!")
        print(f"âœ… Successfully processed: {len(self.processed_files)} files")
        print(f"âŒ Failed: {len(self.failed_files)} files")
        
        return True
    
    def create_batch_summary(self):
        """ãƒãƒƒãƒå‡¦ç†ã®ç·æ‹¬ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆ"""
        summary_path = os.path.join(self.output_base_dir, "batch_summary.md")
        
        with open(summary_path, 'w', encoding='utf-8') as f:
            f.write("# Batch Analysis Summary\n\n")
            f.write(f"**Generated:** {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            f.write(f"## ğŸ“Š Processing Results\n\n")
            f.write(f"- **Total files:** {len(self.processed_files) + len(self.failed_files)}\n")
            f.write(f"- **Successfully processed:** {len(self.processed_files)}\n")
            f.write(f"- **Failed:** {len(self.failed_files)}\n\n")
            
            if self.processed_files:
                f.write(f"## âœ… Successfully Processed Files\n\n")
                f.write("| Shot | File | Output Directory |\n")
                f.write("|------|------|------------------|\n")
                
                for result in self.processed_files:
                    filename = os.path.basename(result['file_path'])
                    output_rel = os.path.relpath(result['output_dir'], self.output_base_dir)
                    f.write(f"| {result['shot_number']} | `{filename}` | `{output_rel}` |\n")
            
            if self.failed_files:
                f.write(f"\n## âŒ Failed Files\n\n")
                f.write("| Shot | File | Error |\n")
                f.write("|------|------|-------|\n")
                
                for result in self.failed_files:
                    filename = os.path.basename(result['file_path'])
                    error = result['error'][:100] + "..." if len(result['error']) > 100 else result['error']
                    f.write(f"| {result['shot_number']} | `{filename}` | {error} |\n")
        
        print(f"ğŸ“‹ Batch summary saved: {summary_path}")

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Batch analysis of multiple Isat data files')
    parser.add_argument('--raw-data-dir', default='raw_data',
                       help='Directory containing raw data files (default: raw_data)')
    parser.add_argument('--output-dir', default='results',
                       help='Base directory for output results (default: results)')
    parser.add_argument('--columns', nargs='*',
                       help='Specific columns to analyze (default: all available)')
    parser.add_argument('--clean-output', action='store_true',
                       help='Clean output directory before starting')
    
    args = parser.parse_args()
    
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    if args.clean_output and os.path.exists(args.output_dir):
        print(f"ğŸ§¹ Cleaning output directory: {args.output_dir}")
        shutil.rmtree(args.output_dir)
    
    # ãƒãƒƒãƒåˆ†æã‚’å®Ÿè¡Œ
    analyzer = BatchAnalyzer(args.raw_data_dir, args.output_dir)
    success = analyzer.run_batch_analysis(args.columns)
    
    if not success:
        sys.exit(1)

if __name__ == "__main__":
    main()